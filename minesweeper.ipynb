{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25196,"status":"ok","timestamp":1664373557268,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"3oWhxMGr59bO","outputId":"6f6a8d6f-b3de-4fe7-ed5b-3a6e2fe2f534"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"minesweeperRL.ipynb\n","​\n","Automatically generated by Colaboratory.\n","​\n","Original file is located at\n","    https://colab.research.google.com/drive/186TEoC7_oHRtUtndAcgbuhH0ZAbzowKp\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('drive/My Drive/rl')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3001,"status":"ok","timestamp":1664373563709,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"IXMgCMp57G1Z"},"outputs":[],"source":["import numpy as np\n","import random\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from typing import NamedTuple\n","from collections import namedtuple"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1664373566976,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"Iyz_nqON9Kzv"},"outputs":[],"source":["Transition = namedtuple(\n","    'Transition', ('state', 'action', 'next_state', 'reward', 'availableAction'))"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":284,"status":"ok","timestamp":1664373569506,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"QfjH5lqh6oyb"},"outputs":[],"source":["class MinesweeperReward(NamedTuple):\n","    win: float = 1.0\n","    lose: float = -1.0\n","    progress: float = 0."]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1664374872491,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"HtPpZdp17Yq8"},"outputs":[],"source":["class MinesweeperEnv:\n","    def __init__(self, row=9, col=9, numMines=10):\n","        self.row = row              # 行（高さ）\n","        self.col = col              # 列（横幅）\n","        self.numMines = numMines    # 地雷の個数\n","\n","        self.mines = np.zeros([self.row, self.col])     # 地雷の位置\n","        self.neighbors = np.zeros([self.row, self.col]) # 隣接する地雷の個数\n","        self.state = np.zeros([self.row, self.col])     # 観測状態\n","        \n","        self.state.fill(np.nan)\n","        self.noOpenCell = np.ones(self.row * self.col) # 開いているマス0, 開いていないマス1\n","        self.reward = MinesweeperReward()   # 報酬\n","        self.initialized = False    # 初期化判定\n","        self.won = False            # 成功判定\n","\n","    # ゲームのリセット\n","    def reset(self):\n","        self.mines.fill(0)\n","        self.neighbors.fill(0)\n","        self.state.fill(np.nan)\n","        self.noOpenCell.fill(1)\n","\n","        self.initialized = False\n","        self.won = False\n","        print(\"state_image in env: \".format(self.state_image()))\n","        return self.state_image()\n","\n","    # 1ステップ進める\n","    def step(self, coordinates):\n","        reward = self.reward.progress\n","        done = False\n","        # 開いているマスを開けた 例外処理\n","        if not np.isnan(self.state[coordinates[0], coordinates[1]]):\n","            print('noprogress')\n","            reward = -1\n","        # 地雷マスを開けた\n","        if self.mines[coordinates[0], coordinates[1]] > 0:\n","            self.state[coordinates[0], coordinates[1]] = -100   # 地雷\n","            reward = self.reward.lose\n","            done = True\n","        # \n","        else:\n","            if not self.initialized:    # 初期化\n","                self.initializeBoard(coordinates)\n","                reward = 0.0\n","            # マスを開く\n","            self.openCell(coordinates)\n","            # 終了判定\n","            if np.sum(np.isnan(self.state)) == self.numMines:\n","                reward = self.reward.win\n","                done = True\n","                self.won = True\n","        # imshow(self.state_image())\n","        return self.state_image(), reward, done, {}\n","    \n","    def state_image(self):\n","        screen_arr = np.zeros((self.row*14, self.col*14))\n","        for i, row in enumerate(self.state):\n","          for j, col in enumerate(row):\n","            # 空いてないマス\n","            if np.isnan(col):\n","              img_arr = image_arr_dict[\"nan\"]\n","            # 地雷\n","            elif int(col) == -100:\n","              img_arr = image_arr_dict[\"mine\"]\n","            # 空いているマス\n","            else:\n","              img_arr = image_arr_dict[str(int(col))]\n","            \n","            w, h = img_arr.shape\n","            screen_arr[i*w:(i+1)*w, j*h:(j+1)*h] += img_arr\n","        \n","        screen_arr = np.ravel(screen_arr)\n","        screen_tensor = torch.tensor(screen_arr, dtype=torch.float)\n","        return screen_tensor\n","\n","    # 盤面の初期化\n","    def initializeBoard(self, coordinates):\n","        # 最初のマスは0\n","        numTotalCells = self.row * self.col\n","        select = coordinates[0]*self.col + coordinates[1]\n","        offLimits = np.array([select-self.col-1, select-self.col, select-self.col+1, \n","        select-1, select, select+1, \n","        select+self.col-1, select+self.col, select+self.col+1])\n","        availableCells = np.setdiff1d(np.arange(numTotalCells), offLimits)\n","        # 最初のマスとその周辺以外に爆弾を配置する\n","        minesFlattend = np.zeros([numTotalCells])\n","        minesFlattend[np.random.choice(availableCells, self.numMines, replace=False)] = 1\n","        self.mines = minesFlattend.reshape([self.row, self.col])\n","        # 隣接する地雷の個数\n","        for row in range(self.row):\n","            for col in range(self.col):\n","                numNeighbors = 0\n","                for i in range(-1, 2):\n","                    if row + i >= 0 and row + i < self.row:\n","                        for j in range(-1, 2):\n","                            if col + j >= 0 and col + j < self.col:\n","                                if not (i == 0 and j == 0):\n","                                    numNeighbors += self.mines[row + i, col + j]\n","                self.neighbors[row, col] = numNeighbors\n","        # 初期化終了\n","        self.initialized = True\n","\n","    # (coordinates[0], coordinates[1]) のマスを開ける\n","    def openCell(self, coordinates):\n","        row = coordinates[0]\n","        col = coordinates[1]\n","        self.state[row, col] = self.neighbors[row, col]\n","        self.noOpenCell[row * self.col + col] = 0   # 開けたマスは0にする\n","        # 0なら周囲の開いていないマスも開ける\n","        if self.state[row, col] == 0:\n","            for i in range(-1, 2):\n","                if row + i >= 0 and row + i < self.row:\n","                    for j in range(-1, 2):\n","                        if col + j >= 0 and col + j < self.col:\n","                            if np.isnan(self.state[row + i, col + j]):\n","                                self.openCell([row + i, col + j])\n","\n","    # ランダムアクション（開いていないマスを開ける）\n","    def randomAction(self):\n","        nonOpenCell = np.array(np.where(self.noOpenCell)).flatten()\n","        action = np.random.choice(nonOpenCell)\n","        return action\n","\n","    # 最初の行動の前に地雷設置\n","    def resetRandomInit(self):\n","        self.mines.fill(0)\n","        self.neighbors.fill(0)\n","        numTotalCells = self.row * self.col\n","        availableCells = np.arange(numTotalCells)\n","        minesFlattend = np.zeros([numTotalCells])\n","        minesFlattend[np.random.choice(availableCells, self.numMines, replace=False)] = 1\n","        self.mines = minesFlattend.reshape([self.row, self.col])\n","        # 隣接する地雷の個数\n","        for row in range(self.row):\n","            for col in range(self.col):\n","                numNeighbors = 0\n","                for i in range(-1, 2):\n","                    if row + i >= 0 and row + i < self.row:\n","                        for j in range(-1, 2):\n","                            if col + j >= 0 and col + j < self.col:\n","                                if not (i == 0 and j == 0):\n","                                    numNeighbors += self.mines[row + i, col + j]\n","                self.neighbors[row, col] = numNeighbors\n","\n","        self.state.fill(np.nan)\n","        self.noOpenCell.fill(1)\n","\n","        self.initialized = True\n","        self.won = False\n","        return self.state\n","\n","    # 固定の地雷配置パターン\n","    def ResetAndSetMines(self, mines):\n","        self.mines = np.copy(mines)\n","        # 隣接する地雷の個数\n","        for row in range(self.row):\n","            for col in range(self.col):\n","                numNeighbors = 0\n","                for i in range(-1, 2):\n","                    if row + i >= 0 and row + i < self.row:\n","                        for j in range(-1, 2):\n","                            if col + j >= 0 and col + j < self.col:\n","                                if not (i == 0 and j == 0):\n","                                    numNeighbors += self.mines[row + i, col + j]\n","                self.neighbors[row, col] = numNeighbors\n","\n","        self.state.fill(np.nan)\n","        self.noOpenCell.fill(1)\n","        \n","        self.initialized = True\n","        self.won = False\n","        return self.state"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":397,"status":"ok","timestamp":1664373575301,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"B4AS1A2p8MHD"},"outputs":[],"source":["class ReplayMemory:\n","    def __init__(self, CAPACITY):\n","        self.capacity = CAPACITY  # メモリの最大長さ\n","        self.memory = []  # 経験を保存する変数\n","        self.index = 0  # 保存するindexを示す変数\n","\n","    def push(self, state, action, state_next, reward, availableAction):\n","        '''transition = (state, action, state_next, reward)をメモリに保存する'''\n","        if len(self.memory) < self.capacity:\n","            self.memory.append(None)  # メモリが満タンでないときは足す\n","        self.memory[self.index] = Transition(state, action, state_next, reward, availableAction)\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size):\n","        '''batch_size分だけ、ランダムに保存内容を取り出す'''\n","        return random.sample(self.memory, batch_size)\n","        \n","    def __len__(self):\n","        '''関数lenに対して、現在の変数memoryの長さを返す'''\n","        return len(self.memory)"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":352,"status":"ok","timestamp":1664373577274,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"J7neg7vp9erV"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self, n_in, n_mid, n_out):\n","        super(Net, self).__init__()\n","        self.fc1 = nn.Linear(n_in, n_mid)\n","        self.fc2 = nn.Linear(n_mid, n_mid)\n","        self.fc3 = nn.Linear(n_mid, n_out)\n","        \n","    def forward(self, x):\n","        h1 = F.relu(self.fc1(x))\n","        h2 = F.relu(self.fc2(h1))\n","        output = self.fc3(h2)\n","        return output"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":1254,"status":"ok","timestamp":1664374623398,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"-UeHv9ja9pYj"},"outputs":[],"source":["GAMMA = 0.9  # 時間割引率\n","BATCH_SIZE = 400\n","CAPACITY = 10**6\n","class Brain:\n","    def __init__(self, num_states, num_actions):\n","        self.num_actions = num_actions  # CartPoleの行動（右に左に押す）の2を取得\n","\n","        # 経験を記憶するメモリオブジェクトを生成\n","        self.memory = ReplayMemory(CAPACITY)\n","\n","        # ニューラルネットワークを構築\n","        n_in, n_mid, n_out = num_states, 64, num_actions\n","        self.model = Net(n_in, n_mid, n_out)\n","        print(self.model)  # ネットワークの形を出力\n","\n","        # 最適化手法の設定\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=10**-4)\n","\n","        self.epsilon = 0.5\n","\n","    def replay(self):\n","        '''Experience Replayでネットワークの結合パラメータを学習'''\n","        # -----------------------------------------\n","        # 1. メモリサイズの確認\n","        # -----------------------------------------\n","        # 1.1 メモリサイズがミニバッチより小さい間は何もしない\n","        if len(self.memory) < BATCH_SIZE:\n","            return\n","\n","        # -----------------------------------------\n","        # 2. ミニバッチの作成\n","        # -----------------------------------------\n","        # 2.1 メモリからミニバッチ分のデータを取り出す\n","        transitions = self.memory.sample(BATCH_SIZE)\n","\n","        # 2.2 各変数をミニバッチに対応する形に変形\n","        # transitionsは1stepごとの(state, action, state_next, reward)が、BATCH_SIZE分格納されている\n","        # つまり、(state, action, state_next, reward)×BATCH_SIZE\n","        # これをミニバッチにしたい。つまり\n","        # (state×BATCH_SIZE, action×BATCH_SIZE, state_next×BATCH_SIZE, reward×BATCH_SIZE)にする\n","        batch = Transition(*zip(*transitions))\n","\n","        # 2.3 各変数の要素をミニバッチに対応する形に変形し、ネットワークで扱えるようVariableにする\n","        # 例えばstateの場合、[torch.FloatTensor of size 1x4]がBATCH_SIZE分並んでいるのですが、\n","        # それを torch.FloatTensor of size BATCH_SIZEx4 に変換します\n","        # 状態、行動、報酬、non_finalの状態のミニバッチのVariableを作成\n","        # catはConcatenates（結合）のことです。\n","        state_batch = torch.cat(batch.state)\n","        action_batch = torch.cat(batch.action)\n","        reward_batch = torch.cat(batch.reward)\n","        non_final_next_states = torch.cat([s for s in batch.next_state\n","                                           if s is not None])\n","        availableAction_batch = torch.cat([a for a in batch.availableAction\n","                                           if a is not None])\n","        \n","        # -----------------------------------------\n","        # 3. 教師信号となるQ(s_t, a_t)値を求める\n","        # -----------------------------------------\n","        # 3.1 ネットワークを推論モードに切り替える\n","        self.model.eval()\n","\n","        # 3.2 ネットワークが出力したQ(s_t, a_t)を求める\n","        # self.model(state_batch)は、右左の両方のQ値を出力しており\n","        # [torch.FloatTensor of size BATCH_SIZEx2]になっている。\n","        # ここから実行したアクションa_tに対応するQ値を求めるため、action_batchで行った行動a_tが右か左かのindexを求め\n","        # それに対応するQ値をgatherでひっぱり出す。\n","        state_action_values = self.model(state_batch).gather(1, action_batch)\n","\n","        # 3.3 max{Q(s_t+1, a)}値を求める。ただし次の状態があるかに注意。\n","\n","        # cartpoleがdoneになっておらず、next_stateがあるかをチェックするインデックスマスクを作成\n","        non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                                    batch.next_state)), dtype=torch.bool)\n","        # まずは全部0にしておく\n","        next_state_values = torch.zeros(BATCH_SIZE)\n","\n","        # 次の状態があるindexの最大Q値を求める\n","        # 出力にアクセスし、max(1)で列方向の最大値の[値、index]を求めます\n","        # そしてそのQ値（index=0）を出力します\n","        # detachでその値を取り出します\n","        next_state_values[non_final_mask] = (\n","            self.model(non_final_next_states)+availableAction_batch).max(1)[0].detach()\n","\n","        # 3.4 教師となるQ(s_t, a_t)値を、Q学習の式から求める\n","        expected_state_action_values = reward_batch + GAMMA * next_state_values\n","\n","        # -----------------------------------------\n","        # 4. 結合パラメータの更新\n","        # -----------------------------------------\n","        # 4.1 ネットワークを訓練モードに切り替える\n","        self.model.train()\n","\n","        # 4.2 損失関数を計算する（smooth_l1_lossはHuberloss）\n","        # expected_state_action_valuesは\n","        # sizeが[minbatch]になっているので、unsqueezeで[minibatch x 1]へ\n","        loss = F.smooth_l1_loss(state_action_values,\n","                                expected_state_action_values.unsqueeze(1))\n","        \n","        # 4.3 結合パラメータを更新する\n","        self.optimizer.zero_grad()  # 勾配をリセット\n","        loss.backward()  # バックプロパゲーションを計算\n","        self.optimizer.step()  # 結合パラメータを更新\n","        \n","    def decide_action(self, state, availableAction, env):\n","        '''現在の状態に応じて、行動を決定する'''\n","        # ε-greedy法で徐々に最適行動のみを採用する\n","\n","        if self.epsilon <= np.random.uniform(0, 1):\n","            self.model.eval()  # ネットワークを推論モードに切り替える\n","            with torch.no_grad():\n","                # action = (self.model(state)+availableAction).max(1)[1].view(1, 1)\n","                print(\"model: \".format(self.model(state)))\n","                # action = (self.model(state)).max(1)[1].view(1, 1)\n","                action = (self.model(state))\n","                #print((self.model(state)*availableAction).max(1)[0].detach())\n","        else:\n","            action = torch.LongTensor([[env.randomAction()]])\n","\n","        return action\n","\n","    def updateEpsilon(self):\n","        self.epsilon *= 0.9997\n","        self.epsilon = max(self.epsilon, 0.1)"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":385,"status":"ok","timestamp":1664374630520,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"vZ7tYlXi-PoK"},"outputs":[],"source":["class Agent:\n","    def __init__(self, num_states, num_actions):\n","        '''課題の状態と行動の数を設定する'''\n","        self.brain = Brain(num_states, num_actions)  # エージェントが行動を決定するための頭脳を生成\n","\n","    def update_q_function(self):\n","        '''Q関数を更新する'''\n","        self.brain.replay()\n","\n","    def get_action(self, state, availableAction, env):\n","        '''行動を決定する'''\n","        action = self.brain.decide_action(state, availableAction, env)\n","        return action\n","\n","    def memorize(self, state, action, state_next, reward, availableAction):\n","        '''memoryオブジェクトに、state, action, state_next, rewardの内容を保存する'''\n","        self.brain.memory.push(state, action, state_next, reward, availableAction)\n","        \n","    def updateEpsilon(self):\n","        self.brain.updateEpsilon()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":488,"status":"ok","timestamp":1664373583858,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"MsFd-GGN4PMI"},"outputs":[],"source":["def imshow(img):\n","    _,ret = cv2.imencode('.jpg', img) \n","    i = IPython.display.Image(data=ret)\n","    IPython.display.display(i)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1664373585424,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"P3JFIBrJB7Pb"},"outputs":[],"source":["def obsTransform(obs, row, col):\n","    availableAction = np.zeros((row, col))\n","    availableAction.fill(-np.inf)\n","    state = np.zeros((row, col))\n","    for i in range(row):\n","        for j in range(col):\n","            if np.isnan(obs[i, j]):\n","                availableAction[i, j] = 0\n","                state[i, j] = -1\n","            else:\n","                state[i, j] = obs[i, j]\n","    state = torch.from_numpy(state.flatten()).type(torch.FloatTensor)\n","    state = torch.unsqueeze(state, 0)\n","    availableAction = torch.from_numpy(availableAction.flatten()).type(torch.FloatTensor)\n","    availableAction = torch.unsqueeze(availableAction, 0)\n","    \n","    return state, availableAction"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1664374790592,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"VHjeH3h7CHRa"},"outputs":[],"source":["def main():\n","      \n","    # main\n","    import csv\n","    with open ('data.csv', 'w') as f:\n","        writer = csv.writer(f)\n","        writer.writerow(['episode', 'step', 'win rate'])\n","\n","    row = 6\n","    col = 6\n","    numMines = 6\n","    env = MinesweeperEnv(row, col, numMines)\n","    num_states = row * col * 14 ** 2\n","    num_actions = row * col\n","    agent = Agent(num_states, num_actions)  # 環境内で行動するAgentを生成\n","\n","    # NUM_EPISODES = 500000  # 最大試行回数\n","    NUM_EPISODES = 3\n","    win = 0\n","    lose = 0\n","\n","    step = 0 # ステップ数\n","\n","    for episode in range(1, NUM_EPISODES+1):\n","        print(episode)\n","        ######******   訓練条件   ******######\n","        state = env.reset()\n","        print(\"initial state: \".format(state))\n","\n","        # state, availableAction = obsTransform(obs, row, col)\n","        reward = 0\n","        done = False\n","\n","        while not done:  # 1エピソードのループ\n","            # action = agent.get_action(state, availableAction, env)  # 行動を求める\n","            action = agent.get_action(state, [], env)  # 行動を求める\n","            print(\"action: \".format(action))\n","            coordinates = divmod(action.item(), col)\n","            obs, reward, done, _ = env.step(coordinates)\n","            # print(\"c: \".format(coordinates))\n","            \n","            reward = torch.FloatTensor([reward])  # 報酬0\n","            # state_next, availableAction = obsTransform(obs, row, col)\n","            state_next = env.state_image()\n","            # print(\"ns: \".format(state_next))\n","            # 終了状態なら価値 0\n","            if done:\n","                state_next = None\n","                # availableAction = None\n","            # メモリに経験を追加\n","            # agent.memorize(state, action, state_next, reward, availableAction)\n","            agent.memorize(state, action, state_next, reward, [])\n","            # Experience ReplayでQ関数を更新する\n","            agent.update_q_function()\n","            # 観測の更新\n","            state = state_next\n","            # ステップ +1\n","            step += 1\n","        # イプシロンを減衰\n","        agent.updateEpsilon()\n","\n","        if env.won:\n","            win += 1\n","        else:\n","            lose += 1    \n","        if episode % 100 == 0:\n","            print('==== Episode {} : win {}, lose {} ===='.format(episode, win, lose))\n","            win = 0\n","            lose = 0\n","\n","        if episode % 1000 == 0:\n","            agent.brain.model.eval()\n","            winTest = 0\n","            for i in range(1000):\n","                ######******   テスト条件   ******######\n","                # obs = env.reset()\n","                state = env.reset()\n","                \n","                # state, availableAction = obsTransform(obs, row, col)\n","                done = False\n","                while not done:\n","                    with torch.no_grad():\n","                        value = agent.brain.model(state)\n","                        # action = (value+availableAction).max(1)[1].view(1, 1)\n","                        action = (value).max(1)[1].view(1, 1)\n","                    coordinates = divmod(action.item(), col)\n","                    state_next, reward, done, _ = env.step(coordinates)\n","                    # obs, reward, done, _ = env.step(coordinates)\n","                    # state_next, availableAction = obsTransform(obs, row, col)\n","                    state = state_next\n","                if env.won:\n","                    winTest += 1\n","            print(winTest)\n","            with open ('data.csv', 'a') as f:\n","                writer = csv.writer(f)\n","                writer.writerow([episode, step, winTest])\n","            winTest = 0\n","    torch.save(agent.brain.model.state_dict(), 'model.pth')"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":500},"executionInfo":{"elapsed":255,"status":"error","timestamp":1664374878575,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"MuwqvtQjC1fB","outputId":"4fd69e3a-c343-426f-d397-3e12da01408c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Net(\n","  (fc1): Linear(in_features=7056, out_features=64, bias=True)\n","  (fc2): Linear(in_features=64, out_features=64, bias=True)\n","  (fc3): Linear(in_features=64, out_features=36, bias=True)\n",")\n","1\n","state_image in env: \n","initial state: \n","action: \n","action: \n","model: \n","action: \n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-27-5a8f22cfee4b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 行動を求める\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"action: \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mcoordinates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoordinates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;31m# print(\"c: \".format(coordinates))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"]}],"source":["main()"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":312,"status":"ok","timestamp":1664349664887,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"sFcqyBvvIBQh"},"outputs":[],"source":["smp = np.array([[np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,],\n"," [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,],\n"," [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,],\n"," [np.nan, np.nan, np.nan,  2.,  1.,  1.,],\n"," [np.nan, np.nan, np.nan,  1.,  0.,  0.,],\n"," [np.nan, np.nan, np.nan,  1.,  0.,  0.,],],)"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1664347618076,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"yY-mWsPf1lHu","outputId":"f8b24848-d2d3-43d9-e2d9-0f2c9111b005"},"outputs":[{"name":"stdout","output_type":"stream","text":["minesweeperrl_org.py\n"]}],"source":["!ls"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":787,"status":"ok","timestamp":1664373598236,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"rv9suoEe2TeU"},"outputs":[],"source":["import glob\n","import cv2\n","import IPython\n","import os\n","import numpy as np\n","from PIL import Image"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"elapsed":6300,"status":"ok","timestamp":1664373606193,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"QiQeLvRZ3I1F","outputId":"49b2ee03-ec74-41a6-c412-e6f65b6012e7"},"outputs":[{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APZPiV8aPGGi/HXW/Bd58Zf+EW0aztoXtJf+EdjvsyNDCxjwEL873bcTgYx3FdF+y78TvGHxD1bxXZ+JPGP9t2umXMCaZef2fHbeZGzTjzNiqpG4Ih2tkjp61o698B/F83xW1P4reCvix/Yl3qdtHBJF/YUdztjVIlK5kfByYlbO0EdPro/Bv4M3/wALdW8Qa5qvjP8Atm68Q3Mc9zL/AGcLfbIGlZmwHYHcZScAADH5f//Z","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APaf2odS+Jnwvv5viRqn7a3/AAgvhe/vobPTtM/4VvDqnkTeQSV8wBpG3GKWTLAAZ254GcT/AIJ+fH34mfGvxJ8RNM8cfE//AISvT9AvrOLQNT/sWGx86F3ux53lxxqy+YsUbbXyV6eufRPiN8KP2l/EvjK81v4fftZf8IzpE/l/ZNE/4QSyvfs2I1V/30jBn3OGfkcb8DgCqX7L/wCy5rP7PniTxn4u8R/FL/hKNQ8aX0N5f3H9iLZbJle4d22rK6ne1wTgBQu3gc8f/9k=","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOp/4KffHj9o/wDZx/ZyXxV+z38L9KisF0y3/t7x3dS2zPpJldIY44LZ/mklZnU+YQyqD0J5X3LRFh1z4DeEPEGrWlvPe3+iabcXly1sgaWWS0DuxwByWJP4143+2z+z58V/2yPAdp8M/B/7Qv8Awifg+60i2j17Qp/CUN617cRSiZJRN5yPGBtjG1Tg7eeprs/hJpvj34RfAvTvht8XvinJ4z1PTbxIrPV4dBisFjskhEcMHlo7ZKBT85OTu56V/9k=","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOQ/bK/4KD6V4M/ba8Afs8aT8TLrwR4F8KRw3nxS8RaR4ca8lv5TEsq6agjgkbG0KjMoGGnbJzHipP8AgmX+2VrP7U3xC+M3hHxt4+n8QR2Pi4X3ge3u9HWFbXRvNuUAUiJdv37cbHO/jpwa+6NE+IXh2HS7MvY3JKW0Wf3Kc4Uf7VfNv7Mn7LVj+yJq3xQ8UXHj6XXv+Fl+OBrscCaOLb+z1LXT+UT5z+Yf34G7C/d6c8f/2Q==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AO0+M37R/wAQvDf7UPiX4b6j+0Z/wg/h3TrO2ksJ/wDhEItTzM1vbOYtojMnzGSR9xJA247jHqX7KvxH/wCFgf29/wAZIf8ACwfsn2X/AJk/+yfsG/zv9kebv2/8B8v/AGqp+K/2WfiFc/HXW/jt8Nvj3/wjV9rVnFaywf8ACLQ3myJIoEK7pZdpy0CtkKCOmeuei+BnwM8T/CzxT4p8a+MviavibU/FLWbXdyuhR2Gw26SIDtjdlJIcZwF5XJySTX//2Q==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOR/af8A23/jL4I/b68dfA/W/wBtr/hVHgrRdNsptIu/+FbW+u7riSyspGt9qwmYbzNPJvZiF2be6gfQH/BPb43f8Lj/AOEu/wCM4P8Ahc39nf2f/wA00/4R3+x/M+0/7C/aPN2e+zyP9uul+L3wL/bK8ZfETUPEnwp/bu/4QzQLnyvsHhr/AIVfp2o/Y9sKK/8ApEzh5N8ivJyPl37Rwoql+xv+xp4t/Zi8bfEL4i+PPjn/AMJzrPxEvLO71K8/4RmPTPLmgNyWbZHM6HebjooQLs6HPH//2Q==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APWPiN4D+Jeh6pqvxN+Iv7UVl4G8CRWNuNFt9J023aVCI1ysgliO92+Y4QsxJAAwKw/2Xda+Lvjay8Sa947uNQ1bwVLqMP8AwhmteINHt7W7u0AlDOEjUHYV2nJyM4wetUfix+zF42+MXxMtfilN8dGW2s0ibQtD1TwzHe22nqI1GBG8/lucgsSU5OM5xXoHhhvi34TtLh/i98ZP+EstpWiWwtoPDEFh9mYB8tmNzuBGBjoMV//Z","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AO1/as+H/wAWvh14c1n4/D9tWbwZ4Vt7C3ez8O2/hC2uGWTyERYIndwZHkkBIBHG854Umsz9gdv2kfHXwXvfi9+0d4kk1Ow169tx4Ts7+whSVYEWbzLjCIuFkJULnqI89CCei+PH7OCftQ/EXwjr/wATviNcnwD4bjhmTwRaaXtN7MEG95Z/OwdxwvCcR5AwWLV7H418a+G4fDdtp2nadNBDBNGkMMcCKkaKjAKoDYAAwAB0Ar//2Q==","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APyQ8R+I/ES+ItQVfEF8AL+bAF4/Hzt71t/CXxBr8viaZZddvWH2B+Gu3P8AGnvR4g+EviaXX7+Vbyxw17MRmZ/75/2K2fhb8LfEVl4jllmu7Ig2Tj5ZX/vp/sV//9k=","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AP30t7eKSLe4JJJ/jPqa+GP+C6XjPxh4K+FfgOHwf4r1LS1u/EF0bkWF68XmlIBt3FSM43tj6mjxn/wXS+Ffgrxhqvg+H4D+ILpdL1Ge1Fy2qQRmUo5Uttw23JB4ya+Y/wDgod/wUO8Kftz+FPDfhvw38N9R0B9A1Ge5llvr6OYTCSNUCgKBgjbmv//Z","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"},{"data":{"image/png":"/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCAAOAA4BAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/AOf/AGyf2yPCPwb8Iv8ABn4Mz/CWHxnD8JrzxBqOo+MNZgsX097aBSYISVcXGoy+YjQWrKDIVYkkECus/wCCZH7d/wADf2/Ph3qes+Ivht4N8MeJdHuViu/DkUcTO0YVS1ym/wCZ4izoucDaTgk5FJ+3f/wTH+Hf7fnwO8NeI9Y8TXGj+JvDPg1U8OXduiiN3MW/y7hirMYmbbnauVwSM5Ir1r9hL9hL4W/sCfC68+F/wv1e91GHUr1b3UNQ1OJBcTzhNhZimBjAAC4AXHfNf//Z","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["image_list = glob.glob(\"./assets/original/*.png\")\n","for image_path in image_list:\n","  image = Image.open(image_path)\n","  image = image.resize((14, 14))\n","  gray_image = image.convert(\"L\")\n","  gray_image.save(\"./assets/gray/{}\".format(os.path.basename(image_path)))\n","  imshow(np.array(gray_image))"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":682,"status":"ok","timestamp":1664373608713,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"U_eYn4-S3XuN"},"outputs":[],"source":["gray_image_list = glob.glob(\"./assets/gray/*.png\")\n","image_arr_dict = {}\n","for image_path in gray_image_list:\n","  image = Image.open(image_path)\n","  arr = np.array(image)\n","  image_arr_dict[os.path.basename(image_path).split(\".\")[0]] = arr"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":491,"status":"ok","timestamp":1664350823412,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"Fz6W4sbO7YeL","outputId":"7094de6a-dae8-492c-d07b-3bbb79f5e713"},"outputs":[{"name":"stdout","output_type":"stream","text":["0 0 nan  0 1 nan  0 2 nan  0 3 nan  0 4 nan  0 5 nan  \n","1 0 nan  1 1 nan  1 2 nan  1 3 nan  1 4 nan  1 5 nan  \n","2 0 nan  2 1 nan  2 2 nan  2 3 nan  2 4 nan  2 5 nan  \n","3 0 nan  3 1 nan  3 2 nan  3 3 2.0  3 4 1.0  3 5 1.0  \n","4 0 nan  4 1 nan  4 2 nan  4 3 1.0  4 4 0.0  4 5 0.0  \n","5 0 nan  5 1 nan  5 2 nan  5 3 1.0  5 4 0.0  5 5 0.0  \n"]}],"source":["screen_arr = np.zeros((84, 84))\n","for i, row in enumerate(smp):\n","  for j, col in enumerate(row):\n","    if np.isnan(col):\n","      img_arr = image_arr_dict[\"nan\"]\n","    else:\n","      img_arr = image_arr_dict[str(int(col))]\n","    \n","    w, h = img_arr.shape\n","    screen_arr[i*w:(i+1)*w, j*h:(j+1)*h] += img_arr\n","  print()"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1664350857651,"user":{"displayName":"服部創","userId":"09612430452687411016"},"user_tz":-540},"id":"fybHNHDJ9i5S","outputId":"eabc86ed-9255-47af-c59a-893762def7ff"},"outputs":[{"data":{"image/png":"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","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"output_type":"display_data"}],"source":["imshow(screen_arr)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UaiHRoSECnJw"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMYG0pLLPtJbzjPfk9DlKCK","provenance":[]},"kernelspec":{"display_name":"Python 3.10.4 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.4"},"vscode":{"interpreter":{"hash":"e3a2d55e2b7847c21802bd14d7498b583363bcf78edda0f72765dd1843b5d1bb"}}},"nbformat":4,"nbformat_minor":0}
